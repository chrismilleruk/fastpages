{
  
    
        "post0": {
            "title": "Evaluating Hyperparameter sweeps with W&B",
            "content": "import pandas as pd import numpy as np import seaborn as sns from sklearn import model_selection from sklearn.model_selection import train_test_split, learning_curve, KFold, cross_val_score, StratifiedKFold # Models import tensorflow from tensorflow import keras from keras import backend as K from keras import regularizers from keras.models import Sequential, model_from_json from keras.preprocessing.image import ImageDataGenerator from keras.utils import np_utils from keras.layers import Dense, Flatten, Conv2D, BatchNormalization, Dropout, MaxPooling2D, Activation from keras.optimizers import RMSprop, SGD, Adam, Nadam from keras import callbacks from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, Callback, EarlyStopping # Image Libraries from PIL import Image, ImageFilter, ImageStat import random random.seed(42) import imageio import PIL import os import itertools import glob import cv2, glob import matplotlib.pyplot as plt from mpl_toolkits.axes_grid1 import AxesGrid %matplotlib inline # Ignore excessive warnings os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;3&#39; import tensorflow as tf tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) . Setup . Start out by installing the experiment tracking library and setting up your free W&amp;B account: . pip install wandb – Install the W&amp;B library | import wandb – Import the wandb library | . %pip install wandb -q import wandb from wandb.keras import WandbCallback . Explore The Simpsons Dataset . !git clone https://github.com/lavanyashukla/simpsons-dataset.git . fatal: destination path &#39;simpsons-dataset&#39; already exists and is not an empty directory. . characters = glob.glob(&#39;simpsons-dataset/kaggle_simpson_testset/kaggle_simpson_testset/**&#39;) # characters = glob.glob(&#39;simpsons-dataset/simpsons_dataset/simpsons_dataset/*/**&#39;) plt.figure(figsize=(16,11)) plt.subplots_adjust(wspace=0, hspace=0.1) i = 0 for character in characters[:15]: img = cv2.imread(character) img = cv2.resize(img, (250, 250)) plt.subplot(3, 5, i+1) plt.title(character.split(&#39;testset/&#39;)[-1]) # plt.title(character.split(&#39;/&#39;)[-2]) plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) plt.axis(&#39;off&#39;) i += 1 . character_names = {0: &#39;abraham_grampa_simpson&#39;, 1: &#39;apu_nahasapeemapetilon&#39;, 2: &#39;bart_simpson&#39;, 3: &#39;charles_montgomery_burns&#39;, 4: &#39;chief_wiggum&#39;, 5: &#39;comic_book_guy&#39;, 6: &#39;edna_krabappel&#39;, 7: &#39;homer_simpson&#39;, 8: &#39;kent_brockman&#39;, 9: &#39;krusty_the_clown&#39;, 10: &#39;lenny_leonard&#39;, 11:&#39;lisa_simpson&#39;, 12: &#39;marge_simpson&#39;, 13: &#39;mayor_quimby&#39;,14:&#39;milhouse_van_houten&#39;, 15: &#39;moe_szyslak&#39;, 16: &#39;ned_flanders&#39;, 17: &#39;nelson_muntz&#39;, 18: &#39;principal_skinner&#39;, 19: &#39;sideshow_bob&#39;} img_size = 64 num_classes = 20 dir = &quot;simpsons-dataset/simpsons_dataset/simpsons_dataset&quot; # Load training data X_train = [] y_train = [] for label, name in character_names.items(): list_images = os.listdir(dir+&#39;/&#39;+name) for image_name in list_images: image = imageio.imread(dir+&#39;/&#39;+name+&#39;/&#39;+image_name) X_train.append(cv2.resize(image, (img_size,img_size))) y_train.append(label) X_train = np.array(X_train) y_train = np.array(y_train) # Split data for cross validation X_test = X_train[-100:] y_test = y_train[-100:] X_train = X_train[:-100] y_train = y_train[:-100] # Normalize the data X_train = X_train / 255.0 X_test = X_test / 255.0 # One hot encode the labels (neural nets only like numbers) y_train = np_utils.to_categorical(y_train, num_classes) y_test = np_utils.to_categorical(y_test, num_classes) . len(X_train), len(y_train), len(X_test), len(y_test) . (19448, 19448, 100, 100) . plt.figure(figsize=(16,10)) plt.subplots_adjust(wspace=0, hspace=0.1) p = 1 for i in range(0, len(X_train), len(X_train)//14): img = X_train[i] label = character_names[y_train[i].argmax(0)] img = cv2.resize(img, (250, 250)) plt.subplot(3, 5, p) p += 1 plt.title(label) plt.imshow(img) plt.axis(&#39;off&#39;) . Run A Sweep . I ran a hyperparameter sweep in the weights and biases tool with 32 runs and you can view the report here: wandb.ai/chrismilleruk/reports/Exploring-W-B-Sweeps . Here you can see the bayesian algorithm gradually improved it&#39;s prediction of which combination of hyperparameters to attempt. . Here are all the hyperparameters laid out in an (interactive) visualisation. . A filtered view of the hyperparameters that yielded &gt;80% accuracy (with &gt;90% highlighted) allows some conclusions to be drawn if further sweeps are required. . Retrieve the best model from the W&amp;B sweep . We&#39;ll just go ahead and get the best model from all the epochs of all the runs in the sweep. . entity = &#39;sweep&#39; project = &#39;simpsons&#39; sweep_id = &quot;uqg7jmld&quot; import wandb api = wandb.Api() sweep = api.sweep(entity + &quot;/&quot; + project + &quot;/&quot; + sweep_id) runs = sorted(sweep.runs, key=lambda run: run.summary.get(&quot;val_accuracy&quot;, 0), reverse=True) val_acc = runs[0].summary.get(&quot;val_accuracy&quot;, 0) print(f&quot;Best run {runs[0].name} with {val_acc * 100}% validation accuracy&quot;) runs[0].file(&quot;model-best.h5&quot;).download(replace=True) print(&quot;Best model saved to model-best.h5&quot;) . Best run gallant-sweep-26 with 99.00000095367432% validation accuracy Best model saved to model-best.h5 . Load the model . model = tf.keras.models.load_model(&#39;model-best.h5&#39;) # Show the model architecture model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 64, 64, 32) 896 _________________________________________________________________ batch_normalization (BatchNo (None, 64, 64, 32) 128 _________________________________________________________________ dropout (Dropout) (None, 64, 64, 32) 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 64, 64, 64) 18496 _________________________________________________________________ batch_normalization_1 (Batch (None, 64, 64, 64) 256 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 32, 32, 64) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 32, 32, 128) 73856 _________________________________________________________________ batch_normalization_2 (Batch (None, 32, 32, 128) 512 _________________________________________________________________ dropout_1 (Dropout) (None, 32, 32, 128) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 32, 32, 128) 147584 _________________________________________________________________ batch_normalization_3 (Batch (None, 32, 32, 128) 512 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 16, 16, 128) 0 _________________________________________________________________ conv2d_4 (Conv2D) (None, 16, 16, 256) 295168 _________________________________________________________________ batch_normalization_4 (Batch (None, 16, 16, 256) 1024 _________________________________________________________________ dropout_2 (Dropout) (None, 16, 16, 256) 0 _________________________________________________________________ conv2d_5 (Conv2D) (None, 16, 16, 256) 590080 _________________________________________________________________ batch_normalization_5 (Batch (None, 16, 16, 256) 1024 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256) 0 _________________________________________________________________ conv2d_6 (Conv2D) (None, 8, 8, 512) 1180160 _________________________________________________________________ batch_normalization_6 (Batch (None, 8, 8, 512) 2048 _________________________________________________________________ dropout_3 (Dropout) (None, 8, 8, 512) 0 _________________________________________________________________ conv2d_7 (Conv2D) (None, 8, 8, 512) 2359808 _________________________________________________________________ batch_normalization_7 (Batch (None, 8, 8, 512) 2048 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 4, 4, 512) 0 _________________________________________________________________ conv2d_8 (Conv2D) (None, 4, 4, 1024) 4719616 _________________________________________________________________ batch_normalization_8 (Batch (None, 4, 4, 1024) 4096 _________________________________________________________________ dropout_4 (Dropout) (None, 4, 4, 1024) 0 _________________________________________________________________ conv2d_9 (Conv2D) (None, 4, 4, 1024) 9438208 _________________________________________________________________ batch_normalization_9 (Batch (None, 4, 4, 1024) 4096 _________________________________________________________________ max_pooling2d_4 (MaxPooling2 (None, 2, 2, 1024) 0 _________________________________________________________________ flatten (Flatten) (None, 4096) 0 _________________________________________________________________ dense (Dense) (None, 512) 2097664 _________________________________________________________________ batch_normalization_10 (Batc (None, 512) 2048 _________________________________________________________________ dropout_5 (Dropout) (None, 512) 0 _________________________________________________________________ dense_1 (Dense) (None, 20) 10260 ================================================================= Total params: 20,949,588 Trainable params: 20,940,692 Non-trainable params: 8,896 _________________________________________________________________ . Make some predictions with the model . def get_prediction(x, y): # Resize image and normalize it pic = cv2.resize(x, (64, 64)).astype(&#39;float32&#39;) if pic.max() &gt; 1.: pic = pic / 255. # Get predictions for the character prediction = model.predict(pic.reshape(1, 64, 64, 3))[0] # Get true name of the character character = character_names[y] name = character.split(&#39;_&#39;)[0].title() # Format predictions to string to overlay on image text = sorted([&#39;{:s} : {:.1f}%&#39;.format(character_names[k].split(&#39;_&#39;)[0].title(), 100*v) for k,v in enumerate(prediction)], key=lambda x:float(x.split(&#39;:&#39;)[1].split(&#39;%&#39;)[0]), reverse=True)[:3] # Upscale original image (expecting a 0-255 range here) img = cv2.resize(x, (352, 352)) if np.issubdtype(img.dtype, &#39;float&#39;): img = (img * 255).astype(&#39;uint8&#39;) # Create background to overlay text on cv2.rectangle(img, (0,260),(215,352),(255,255,255), -1) # Add text to image font = cv2.FONT_HERSHEY_DUPLEX cv2.putText(img, &#39;Name : %s&#39; % name, (10, 280), font, 0.7,(73,79,183), 2, cv2.LINE_AA) for k, t in enumerate(text): color = (10,100,10) if name in t else (80,0,0) cv2.putText(img, t, (10, 300+k*18), font, 0.65, color, 2, cv2.LINE_AA) title = &quot;%s: %s&quot; % (name, text[0]) return img, title . Visualisation of performance against the validation data . This should be very good since it was used during training. This is also how we measured the success of the model vs other models. The best run was had a 99% validation accuracy so anything less than that would be a problem here. . plt.figure(figsize=(18,8)) plt.subplots_adjust(wspace=0, hspace=0.1) p = 1 for i in range(0, len(X_test), len(X_test) // 9): plt.subplot(2, 5, p) p += 1 x = X_test[i] y = y_test[i].argmax() (img, label) = get_prediction(x, y) plt.imshow( img ) plt.title( label ) plt.axis(&#39;off&#39;) . The predictions are all great. Very confident and correct. . But there is a problem here. While the predictions are all great it looks like the validation data contains only pictures of Sideshow Bob. . This is a problem that needs to be fixed but how will the model perform against a dataset it&#39;s never seen before? . Visualize predictions against the kaggle testset . This dataset was never used during training and so provides a view of the model performance under exam conditions. . def predict_test(): predicted_images = [] for i in range(20): character = character_names[i] # Read in a character image from the test dataset image = cv2.imread(np.random.choice([k for k in glob.glob(&#39;simpsons-dataset/kaggle_simpson_testset/kaggle_simpson_testset/*.*&#39;) if character in k])) # print(image.shape) image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) (img, title) = get_prediction(image, i) predicted_images.append((img, title)) return predicted_images . predicted = predict_test() plt.figure(figsize=(18,18)) plt.subplots_adjust(wspace=0, hspace=0) p = 1 for i in range(0, len(predicted)): img = predicted[i][0] label = predicted[i][1] img = cv2.resize(img, (250, 250)) plt.subplot(4, 5, p) p += 1 plt.title(label) plt.imshow(img) plt.axis(&#39;off&#39;) . Not bad considering the flaw in our methodology. I&#39;ve run this a few times now and we occasionally drop one or two for each batch of 25. It&#39;s probably around 90% at a guess. . Let&#39;s check the whole kaggle test set. . testset = glob.glob(&#39;simpsons-dataset/kaggle_simpson_testset/kaggle_simpson_testset/*.jpg&#39;) img_size = 64 x_testset = [] y_testset = [] for i in range(len(testset)): path = testset[i] image = imageio.imread(path) image = cv2.resize(image, (img_size,img_size)) if image.shape != (img_size, img_size, 3): continue filename = path.split(&#39;testset/&#39;)[-1] names = [k for k, v in character_names.items() if v in filename] if not names: continue x_testset.append(image) y_testset.append(names[0]) x_testset = np.array(x_testset) y_testset = np.array(y_testset) # Normalise image data x_testset = x_testset / 255 print(&#39;Making model predictions for %d&#39; % len(x_testset)) prediction = model.predict(x_testset) print(&#39;Compare first 20 predictions:&#39;) print(prediction.argmax(1)[:20]) print(y_testset[:20]) arr = prediction.argmax(1) == y_testset print(np.sum(arr), len(arr)) print(&#39;Accuracy&#39;, np.sum(arr) / len(arr) * 100, &#39;%&#39;) # # Get predictions for the character # prediction = model.predict(x.reshape(1, 64, 64, 3))[0] # predicted_id = prediction.argmax() # predicted_confidence = prediction[predicted_id] * 100 # print(filename, y, character_names[y], predicted_id == y, predicted_confidence) . Making model predictions for 938 Compare first 20 predictions: [11 17 10 19 4 15 9 17 4 19 15 8 3 0 1 17 14 17 11 8] [11 17 10 19 4 15 9 10 4 0 15 8 3 0 1 10 14 17 11 8] 806 938 Accuracy 85.9275053304904 % . 85% accuracy across 938 test images. . Not bad, but could be improved. . We have some paths to make this better: . Correct the flaw in the validation data. | Run a broad/narrow sweep | Test again. |",
            "url": "https://chrismilleruk.github.io/fastpages/2021/01/24/Eval_Hyperparameter_Sweeps_with_W&B.html",
            "relUrl": "/2021/01/24/Eval_Hyperparameter_Sweeps_with_W&B.html",
            "date": " • Jan 24, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Introduction to Hyperparameter Sweeps – A Battle Royale To Find The Best Model In 3 Steps",
            "content": "Let&#39;s Dive In! . You know how sweeps work on a fundamental level. Now let&#39;s use them with a real model. . import pandas as pd import numpy as np import seaborn as sns from sklearn import model_selection from sklearn.model_selection import train_test_split, learning_curve, KFold, cross_val_score, StratifiedKFold # Models import tensorflow from tensorflow import keras from keras import backend as K from keras import regularizers from keras.models import Sequential, model_from_json from keras.preprocessing.image import ImageDataGenerator from keras.utils import np_utils from keras.layers import Dense, Flatten, Conv2D, BatchNormalization, Dropout, MaxPooling2D, Activation from keras.optimizers import RMSprop, SGD, Adam, Nadam from keras import callbacks from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, Callback, EarlyStopping # Image Libraries from PIL import Image, ImageFilter, ImageStat import random random.seed(42) import imageio import PIL import os import itertools import glob import cv2, glob import matplotlib.pyplot as plt from mpl_toolkits.axes_grid1 import AxesGrid %matplotlib inline # Ignore excessive warnings os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;3&#39; import tensorflow as tf tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) . The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x. We recommend you upgrade now or ensure your notebook will continue to use TensorFlow 1.x via the %tensorflow_version 1.x magic: more info. . Using TensorFlow backend. . Setup . Start out by installing the experiment tracking library and setting up your free W&amp;B account: . pip install wandb – Install the W&amp;B library | import wandb – Import the wandb library | . %pip install wandb -q import wandb from wandb.keras import WandbCallback . |████████████████████████████████| 1.3MB 9.4MB/s |████████████████████████████████| 256kB 74.3MB/s |████████████████████████████████| 460kB 70.5MB/s |████████████████████████████████| 92kB 14.3MB/s |████████████████████████████████| 92kB 12.6MB/s |████████████████████████████████| 102kB 13.8MB/s |████████████████████████████████| 184kB 70.5MB/s |████████████████████████████████| 71kB 11.6MB/s Building wheel for shortuuid (setup.py) ... done Building wheel for watchdog (setup.py) ... done Building wheel for gql (setup.py) ... done Building wheel for subprocess32 (setup.py) ... done Building wheel for pathtools (setup.py) ... done . Explore The Simpsons Dataset . !git clone https://github.com/lavanyashukla/simpsons-dataset.git . Cloning into &#39;simpsons-dataset&#39;... remote: Enumerating objects: 21934, done. remote: Total 21934 (delta 0), reused 0 (delta 0), pack-reused 21934 Receiving objects: 100% (21934/21934), 590.28 MiB | 37.97 MiB/s, done. Resolving deltas: 100% (1/1), done. Checking out files: 100% (42860/42860), done. . characters = glob.glob(&#39;simpsons-dataset/kaggle_simpson_testset/kaggle_simpson_testset/**&#39;) plt.figure(figsize=(10,10)) plt.subplots_adjust(wspace=0, hspace=0) i = 0 for character in characters[:25]: img = cv2.imread(character) img = cv2.resize(img, (250, 250)) plt.axis(&#39;off&#39;) plt.subplot(5, 5, i+1) #.set_title(l) plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) i += 1 . character_names = {0: &#39;abraham_grampa_simpson&#39;, 1: &#39;apu_nahasapeemapetilon&#39;, 2: &#39;bart_simpson&#39;, 3: &#39;charles_montgomery_burns&#39;, 4: &#39;chief_wiggum&#39;, 5: &#39;comic_book_guy&#39;, 6: &#39;edna_krabappel&#39;, 7: &#39;homer_simpson&#39;, 8: &#39;kent_brockman&#39;, 9: &#39;krusty_the_clown&#39;, 10: &#39;lenny_leonard&#39;, 11:&#39;lisa_simpson&#39;, 12: &#39;marge_simpson&#39;, 13: &#39;mayor_quimby&#39;,14:&#39;milhouse_van_houten&#39;, 15: &#39;moe_szyslak&#39;, 16: &#39;ned_flanders&#39;, 17: &#39;nelson_muntz&#39;, 18: &#39;principal_skinner&#39;, 19: &#39;sideshow_bob&#39;} img_size = 64 num_classes = 20 dir = &quot;simpsons-dataset/simpsons_dataset/simpsons_dataset&quot; # Load training data X_train = [] y_train = [] for label, name in character_names.items(): list_images = os.listdir(dir+&#39;/&#39;+name) for image_name in list_images: image = imageio.imread(dir+&#39;/&#39;+name+&#39;/&#39;+image_name) X_train.append(cv2.resize(image, (img_size,img_size))) y_train.append(label) X_train = np.array(X_train) y_train = np.array(y_train) # Split data for cross validation X_train = X_train[:1000] y_train = y_train[:1000] X_test = X_train[-100:] y_test = y_train[-100:] # Normalize the data X_train = X_train / 255.0 X_test = X_test / 255.0 # One hot encode the labels (neural nets only like numbers) y_train = np_utils.to_categorical(y_train, num_classes) y_test = np_utils.to_categorical(y_test, num_classes) . Run A Sweep . As you&#39;ll recall there are just 3 simple steps to running a sweep: . 1. Define the sweep . 2. Initialize the sweep . 3. Run the sweep agent . Let&#39;s walk through each step in more detail. . . 1. Define the Sweep . Weights &amp; Biases sweeps give you powerful levers to configure your sweeps exactly how you want them, with just a few lines of code. The sweeps config can be defined as a dictionary or a YAML file. . Let&#39;s walk through some of them together: . Metric – This is the metric the sweeps are attempting to optimize. Metrics can take a name (this metric should be logged by your training script) and a goal (maximize or minimize). | Search Strategy – Specified using the &#39;method&#39; variable. We support several different search strategies with sweeps. Grid Search – Iterates over every combination of hyperparameter values. | Random Search – Iterates over randomly chosen combinations of hyperparameter values. | Bayesian Search – Creates a probabilistic model that maps hyperparameters to probability of a metric score, and chooses parameters with high probability of improving the metric. The objective of Bayesian optimization is to spend more time in picking the hyperparameter values, but in doing so trying out fewer hyperparameter values. | . | Stopping Criteria – The strategy for determining when to kill off poorly peforming runs, and try more combinations faster. We offer several custom scheduling algorithms like HyperBand and Envelope. | Parameters – A dictionary containing the hyperparameter names, and discreet values, max and min values or distributions from which to pull their values to sweep over. | . You can find a list of all configuration options here. . A Note To Advanced Users . Advanced users can modify sweep algorithms or write their own based on the W&amp;B base classes wandb.sweeps.base.Search and wandb.sweeps.base.EarlyTerminate, which you can find in our open source client library. . They can also create a local controller, which lets users take full control of search and stopping criteria, inspect and modify the code in order to debug issues as well as develop new features which can be incorporated into the cloud service. . sweep_config = { &#39;method&#39;: &#39;random&#39;, #grid, random &#39;metric&#39;: { &#39;name&#39;: &#39;accuracy&#39;, &#39;goal&#39;: &#39;maximize&#39; }, &#39;parameters&#39;: { &#39;epochs&#39;: { &#39;values&#39;: [10, 20, 50] }, &#39;dropout&#39;: { &#39;values&#39;: [0.3, 0.4, 0.5] }, &#39;conv_layer_size&#39;: { &#39;values&#39;: [16, 32, 64] }, &#39;encoder_size&#39;: { &#39;values&#39;: [128, 256, 512] }, &#39;decoder_size&#39;: { &#39;values&#39;: [256, 512, 1024] }, &#39;weight_decay&#39;: { &#39;values&#39;: [0.0005, 0.005, 0.05] }, &#39;learning_rate&#39;: { &#39;values&#39;: [1e-2, 1e-3, 1e-4, 3e-4, 3e-5, 1e-5] }, &#39;optimizer&#39;: { &#39;values&#39;: [&#39;adam&#39;, &#39;nadam&#39;, &#39;sgd&#39;, &#39;rmsprop&#39;] }, &#39;activation&#39;: { &#39;values&#39;: [&#39;relu&#39;, &#39;elu&#39;, &#39;selu&#39;, &#39;softmax&#39;] }, &#39;layer&#39;: { &#39;values&#39;: [&#39;LSTM&#39;, &#39;GRU&#39;] } } } . 2. Initialize the Sweep . # Arguments: # – sweep_config: the sweep config dictionary defined above # – entity: Set the username for the sweep # – project: Set the project name for the sweep sweep_id = wandb.sweep(sweep_config, entity=&quot;sweep&quot;, project=&quot;simpsons&quot;) . Create sweep with ID: bfozkh0e Sweep URL: https://app.wandb.ai/sweep/simpsons/sweeps/bfozkh0e . Define Your Neural Network . Before we can run the sweep, let&#39;s define a function that creates and trains our neural network. . In the function below, we define a simplified version of a VGG19 model in Keras, and add the following lines of code to log models metrics, visualize performance and output and track our experiments easily: . wandb.init() – Initialize a new W&amp;B run. Each run is single execution of the training script. | wandb.config – Save all your hyperparameters in a config object. This lets you use our app to sort and compare your runs by hyperparameter values. | callbacks=[WandbCallback()] – Fetch all layer dimensions, model parameters and log them automatically to your W&amp;B dashboard. | wandb.log() – Logs custom objects – these can be images, videos, audio files, HTML, plots, point clouds etc. Here we use wandb.log to log images of Simpson characters overlaid with actual and predicted labels. | . def train(): # Default values for hyper-parameters we&#39;re going to sweep over config_defaults = { &#39;epochs&#39;: 2, &#39;batch_size&#39;: 128, &#39;weight_decay&#39;: 0.0005, &#39;learning_rate&#39;: 1e-3, &#39;activation&#39;: &#39;relu&#39;, &#39;optimizer&#39;: &#39;nadam&#39;, &#39;seed&#39;: 42 } # Initialize a new wandb run wandb.init(config=config_defaults) # Config is a variable that holds and saves hyperparameters and inputs config = wandb.config # Determine input shape input_shape = (X_train.shape[1], X_train.shape[2], 3) # Define the model architecture - This is a simplified version of the VGG19 architecture model = Sequential() # Set of Conv2D, Conv2D, MaxPooling2D layers with 32 and 64 filters model.add(Conv2D(filters = 32, kernel_size = (3, 3), padding = &#39;same&#39;, activation =&#39;relu&#39;, input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay))) model.add(BatchNormalization()) model.add(Dropout(0.3)) model.add(Conv2D(filters = 64, kernel_size = (3, 3), padding = &#39;same&#39;, activation =&#39;relu&#39;, input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay))) model.add(BatchNormalization()) model.add(MaxPooling2D(pool_size=(2, 2))) # Another set of Conv2D, Conv2D, MaxPooling2D layers with 128 filters model.add(Conv2D(filters = 128, kernel_size = (3, 3), padding = &#39;same&#39;, activation =&#39;relu&#39;, input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay))) model.add(BatchNormalization()) model.add(Dropout(0.4)) model.add(Conv2D(filters = 128, kernel_size = (3, 3), padding = &#39;same&#39;, activation =&#39;relu&#39;, input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay))) model.add(BatchNormalization()) model.add(MaxPooling2D(pool_size=(2, 2))) # Another set of Conv2D, Conv2D, MaxPooling2D layers with 256 filters model.add(Conv2D(filters = 256, kernel_size = (3, 3), padding = &#39;same&#39;, activation =&#39;relu&#39;, input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay))) model.add(BatchNormalization()) model.add(Dropout(0.4)) model.add(Conv2D(filters = 256, kernel_size = (3, 3), padding = &#39;same&#39;, activation =&#39;relu&#39;, input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay))) model.add(BatchNormalization()) model.add(MaxPooling2D(pool_size=(2, 2))) # Another set of Conv2D, Conv2D, MaxPooling2D layers with 512 filters model.add(Conv2D(filters = 512, kernel_size = (3, 3), padding = &#39;same&#39;, activation =&#39;relu&#39;, input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay))) model.add(BatchNormalization()) model.add(Dropout(0.4)) model.add(Conv2D(filters = 512, kernel_size = (3, 3), padding = &#39;same&#39;, activation =&#39;relu&#39;, input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay))) model.add(BatchNormalization()) model.add(MaxPooling2D(pool_size=(2, 2))) # Another set of Conv2D, Conv2D, MaxPooling2D layers with 512 filters model.add(Conv2D(filters = 1024, kernel_size = (3, 3), padding = &#39;same&#39;, activation =&#39;relu&#39;, input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay))) model.add(BatchNormalization()) model.add(Dropout(0.4)) model.add(Conv2D(filters = 1024, kernel_size = (3, 3), padding = &#39;same&#39;, activation =&#39;relu&#39;, input_shape = input_shape, kernel_regularizer=regularizers.l2(config.weight_decay))) model.add(BatchNormalization()) model.add(MaxPooling2D(pool_size=(2, 2))) # Flatten model.add(Flatten()) model.add(Dense(512, activation =&#39;relu&#39;, kernel_regularizer=regularizers.l2(config.weight_decay))) model.add(BatchNormalization()) model.add(Dropout(0.5)) model.add(Dense(num_classes, activation = &quot;softmax&quot;)) # Define the optimizer if config.optimizer==&#39;sgd&#39;: optimizer = SGD(lr=config.learning_rate, decay=1e-5, momentum=0.9, nesterov=True) elif config.optimizer==&#39;rmsprop&#39;: optimizer = RMSprop(lr=config.learning_rate, decay=1e-5) elif config.optimizer==&#39;adam&#39;: optimizer = Adam(lr=config.learning_rate, beta_1=0.9, beta_2=0.999, clipnorm=1.0) elif config.optimizer==&#39;nadam&#39;: optimizer = Nadam(lr=config.learning_rate, beta_1=0.9, beta_2=0.999, clipnorm=1.0) model.compile(loss = &quot;categorical_crossentropy&quot;, optimizer = optimizer, metrics=[&#39;accuracy&#39;]) #data augmentation datagen = ImageDataGenerator( featurewise_center=False, # set input mean to 0 over the dataset samplewise_center=False, # set each sample mean to 0 featurewise_std_normalization=False, # divide inputs by std of the dataset samplewise_std_normalization=False, # divide each input by its std zca_whitening=False, # apply ZCA whitening rotation_range=15, # randomly rotate images in the range (degrees, 0 to 180) width_shift_range=0.1, # randomly shift images horizontally (fraction of total width) height_shift_range=0.1, # randomly shift images vertically (fraction of total height) horizontal_flip=True, # randomly flip images vertical_flip=False) # randomly flip images # (std, mean, and principal components if ZCA whitening is applied). datagen.fit(X_train) model.fit_generator(datagen.flow(X_train, y_train, batch_size=config.batch_size), steps_per_epoch=len(X_train) / 32, epochs=config.epochs, validation_data=(X_test, y_test), callbacks=[WandbCallback(data_type=&quot;image&quot;, validation_data=(X_test, y_test), labels=character_names), keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]) predicted_images = [] for i in range(20): character = character_names[i] # Read in a character image from the test dataset image = cv2.imread(np.random.choice([k for k in glob.glob(&#39;simpsons-dataset/kaggle_simpson_testset/kaggle_simpson_testset/*.*&#39;) if character in k])) img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Resize image and normalize it pic = cv2.resize(image, (64, 64)).astype(&#39;float32&#39;) / 255. # Get predictions for the character prediction = model.predict(pic.reshape(1, 64, 64,3))[0] # Get true name of the character name = character.split(&#39;_&#39;)[0].title() # Format predictions to string to overlay on image text = sorted([&#39;{:s} : {:.1f}%&#39;.format(character_names[k].split(&#39;_&#39;)[0].title(), 100*v) for k,v in enumerate(prediction)], key=lambda x:float(x.split(&#39;:&#39;)[1].split(&#39;%&#39;)[0]), reverse=True)[:3] # Upscale image img = cv2.resize(img, (352, 352)) # Create background to overlay text on cv2.rectangle(img, (0,260),(215,352),(255,255,255), -1) # Add text to image font = cv2.FONT_HERSHEY_DUPLEX cv2.putText(img, &#39;True Name : %s&#39; % name, (10, 280), font, 0.7,(73,79,183), 2, cv2.LINE_AA) for k, t in enumerate(text): cv2.putText(img, t, (10, 300+k*18), font, 0.65,(0,0,0), 2, cv2.LINE_AA) # Add predicted image from test dataset with annotations to array predicted_images.append(wandb.Image(img, caption=&quot;Actual: %s&quot; % name)) # Log images from test set to wandb automatically, along with predicted and true labels by passing pytorch tensors with image data into wandb.Image wandb.log({&quot;predictions&quot;: predicted_images}) . 3. Run the sweep agent . # Arguments: # – sweep_id: the sweep_id to run - this was returned above by wandb.sweep() # – function: function that defines your model architecture and trains it wandb.agent(sweep_id, train) . wandb: Agent Starting Run: qhzyiysj with config: batch_size: 128 epochs: 100 learning_rate: 0.001 optimizer: sgd weight_decay: 0.05 wandb: Agent Started Run: qhzyiysj . Logging results to Weights &amp; Biases (Documentation). Project page: https://app.wandb.ai/sweep/simpsons Sweep page: https://app.wandb.ai/sweep/simpsons/sweeps/bfozkh0e Run page: https://app.wandb.ai/sweep/simpsons/runs/qhzyiysj Epoch 1/100 32/31 [==============================] - 18s 571ms/step - loss: 208.1772 - acc: 0.1782 - val_loss: 202.2435 - val_acc: 0.2900 Epoch 2/100 32/31 [==============================] - 8s 263ms/step - loss: 195.4176 - acc: 0.6672 - val_loss: 189.9604 - val_acc: 0.5700 Epoch 3/100 32/31 [==============================] - 8s 263ms/step - loss: 182.3787 - acc: 0.9039 - val_loss: 177.9252 - val_acc: 0.6500 Epoch 4/100 32/31 [==============================] - 8s 266ms/step - loss: 170.7165 - acc: 0.9417 - val_loss: 170.7456 - val_acc: 0.1800 Epoch 5/100 32/31 [==============================] - 8s 263ms/step - loss: 159.8565 - acc: 0.9722 - val_loss: 159.6783 - val_acc: 0.2200 Epoch 6/100 32/31 [==============================] - 8s 263ms/step - loss: 149.8787 - acc: 0.9744 - val_loss: 147.8720 - val_acc: 0.3800 Epoch 7/100 32/31 [==============================] - 8s 263ms/step - loss: 140.5229 - acc: 0.9801 - val_loss: 138.6446 - val_acc: 0.3300 Epoch 8/100 32/31 [==============================] - 8s 262ms/step - loss: 131.7099 - acc: 0.9859 - val_loss: 130.0943 - val_acc: 0.3300 Epoch 9/100 32/31 [==============================] - 8s 264ms/step - loss: 123.5052 - acc: 0.9865 - val_loss: 120.9473 - val_acc: 0.4800 Epoch 10/100 32/31 [==============================] - 8s 263ms/step - loss: 115.8305 - acc: 0.9851 - val_loss: 114.8688 - val_acc: 0.1800 Epoch 11/100 32/31 [==============================] - 8s 264ms/step - loss: 108.6039 - acc: 0.9881 - val_loss: 106.6842 - val_acc: 0.4300 Epoch 12/100 23/31 [=====================&gt;........] - ETA: 2s - loss: 102.7420 - acc: 0.9903 . Visualize Predictions Live . Project Overview . Check out the project page to see your results in the shared project. | Press &#39;option+space&#39; to expand the runs table, comparing all the results from everyone who has tried this script. | Click on the name of a run to dive in deeper to that single run on its own run page. | . Visualize Sweep Results . Use a parallel coordinates chart to see which hyperparameter values led to the best accuracy. . . We can tweak the slides in the parallel co-ordinates chart to only view the runs that led to the best accuracy values. This can help us hone in on ranges of hyperparameter values to sweep over next. . . Visualize Performance . Click through to a single run to see more details about that run. For example, on this run page you can see the performance metrics I logged when I ran this script. . . Visualize Predictions . You can visualize predictions made at everystep by clicking on the Media tab. . . Review Code . The overview tab picks up a link to the code. In this case, it&#39;s a link to the Google Colab. If you&#39;re running a script from a git repo, we&#39;ll pick up the SHA of the latest git commit and give you a link to that version of the code in your own GitHub repo. . . Visualize System Metrics . The System tab on the runs page lets you visualize how resource efficient your model was. It lets you monitor the GPU, memory, CPU, disk, and network usage in one spot. . . Next Steps . As you can see running sweeps is super easy! We highly encourage you to fork this notebook, tweak the parameters, or try the model with your own dataset! . More about Weights &amp; Biases . We&#39;re always free for academics and open source projects. Email carey@wandb.com with any questions or feature suggestions. Here are some more resources: . Documentation - Python docs | Gallery - example reports in W&amp;B | Articles - blog posts and tutorials | Community - join our Slack community forum |",
            "url": "https://chrismilleruk.github.io/fastpages/2021/01/22/Intro_to_Hyperparameter_Sweeps_with_W&B.html",
            "relUrl": "/2021/01/22/Intro_to_Hyperparameter_Sweeps_with_W&B.html",
            "date": " • Jan 22, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Under the Hood - Training a Digit Classifier",
            "content": "Pixels: The Foundations of Computer Vision . Sidebar: Tenacity and Deep Learning . End sidebar . path = untar_data(URLs.MNIST_SAMPLE) . path.ls() . (#3) [Path(&#39;labels.csv&#39;),Path(&#39;valid&#39;),Path(&#39;train&#39;)] . (path/&#39;train&#39;).ls() . (#2) [Path(&#39;train/7&#39;),Path(&#39;train/3&#39;)] . threes = (path/&#39;train&#39;/&#39;3&#39;).ls().sorted() sevens = (path/&#39;train&#39;/&#39;7&#39;).ls().sorted() threes . (#6131) [Path(&#39;train/3/10.png&#39;),Path(&#39;train/3/10000.png&#39;),Path(&#39;train/3/10011.png&#39;),Path(&#39;train/3/10031.png&#39;),Path(&#39;train/3/10034.png&#39;),Path(&#39;train/3/10042.png&#39;),Path(&#39;train/3/10052.png&#39;),Path(&#39;train/3/1007.png&#39;),Path(&#39;train/3/10074.png&#39;),Path(&#39;train/3/10091.png&#39;)...] . im3_path = threes[1] im3 = Image.open(im3_path) im3 . array(im3)[4:10,4:10] . array([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=uint8) . tensor(im3)[4:10,4:10] . tensor([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=torch.uint8) . im3_t = tensor(im3) df = pd.DataFrame(im3_t[4:15,4:22]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 29 | 150 | 195 | 254 | 255 | 254 | 176 | 193 | 150 | 96 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 48 | 166 | 224 | 253 | 253 | 234 | 196 | 253 | 253 | 253 | 253 | 233 | 0 | 0 | 0 | . 3 0 | 93 | 244 | 249 | 253 | 187 | 46 | 10 | 8 | 4 | 10 | 194 | 253 | 253 | 233 | 0 | 0 | 0 | . 4 0 | 107 | 253 | 253 | 230 | 48 | 0 | 0 | 0 | 0 | 0 | 192 | 253 | 253 | 156 | 0 | 0 | 0 | . 5 0 | 3 | 20 | 20 | 15 | 0 | 0 | 0 | 0 | 0 | 43 | 224 | 253 | 245 | 74 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 249 | 253 | 245 | 126 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 101 | 223 | 253 | 248 | 124 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 11 | 166 | 239 | 253 | 253 | 253 | 187 | 30 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 16 | 248 | 250 | 253 | 253 | 253 | 253 | 232 | 213 | 111 | 2 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 98 | 98 | 208 | 253 | 253 | 253 | 253 | 187 | 22 | 0 | . First Try: Pixel Similarity . seven_tensors = [tensor(Image.open(o)) for o in sevens] three_tensors = [tensor(Image.open(o)) for o in threes] len(three_tensors),len(seven_tensors) . (6131, 6265) . show_image(three_tensors[1]); . stacked_sevens = torch.stack(seven_tensors).float()/255 stacked_threes = torch.stack(three_tensors).float()/255 stacked_threes.shape, stacked_sevens.shape . (torch.Size([6131, 28, 28]), torch.Size([6265, 28, 28])) . len(stacked_threes.shape) . 3 . stacked_threes.ndim . 3 . mean3 = stacked_threes.mean(0) show_image(mean3); . mean7 = stacked_sevens.mean(0) show_image(mean7); . a_3 = stacked_threes[1] show_image(a_3); . dist_3_abs = (a_3 - mean3).abs().mean() dist_3_sqr = ((a_3 - mean3)**2).mean().sqrt() dist_3_abs,dist_3_sqr . (tensor(0.1114), tensor(0.2021)) . dist_7_abs = (a_3 - mean7).abs().mean() dist_7_sqr = ((a_3 - mean7)**2).mean().sqrt() dist_7_abs,dist_7_sqr . (tensor(0.1586), tensor(0.3021)) . F.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt() . (tensor(0.1586), tensor(0.3021)) . NumPy Arrays and PyTorch Tensors . data = [[1,2,3],[4,5,6]] arr = array (data) tns = tensor(data) . arr # numpy . tns # pytorch . tns[1] . tns[:,1] . tns[1,1:3] . tns+1 . tns.type() . tns*1.5 . Computing Metrics Using Broadcasting . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 valid_3_tens.shape,valid_7_tens.shape . (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) . def mnist_distance(a,b): return (a-b).abs().mean((-1,-2)) mnist_distance(a_3, mean3) . tensor(0.1114) . valid_3_dist = mnist_distance(valid_3_tens, mean3) valid_3_dist, valid_3_dist.shape . (tensor([0.1290, 0.1223, 0.1380, ..., 0.1337, 0.1132, 0.1097]), torch.Size([1010])) . tensor([1,2,3]) + tensor([1,1,1]) . tensor([2, 3, 4]) . (valid_3_tens-mean3).shape . torch.Size([1010, 28, 28]) . valid_3_tens.shape, mean3.shape . (torch.Size([1010, 28, 28]), torch.Size([28, 28])) . def is_3(x): return mnist_distance(x,mean3) &lt; mnist_distance(x,mean7) . is_3(a_3), is_3(a_3).float() . (tensor(True), tensor(1.)) . is_3(valid_3_tens) . tensor([True, True, True, ..., True, True, True]) . accuracy_3s = is_3(valid_3_tens).float() .mean() accuracy_7s = (1 - is_3(valid_7_tens).float()).mean() accuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2 . (tensor(0.9168), tensor(0.9854), tensor(0.9511)) . is_3(stacked_threes).float().mean(), 1 - is_3(stacked_sevens).float().mean() . (tensor(0.8912), tensor(0.9962)) . Stochastic Gradient Descent (SGD) . gv(&#39;&#39;&#39; init-&gt;predict-&gt;loss-&gt;gradient-&gt;step-&gt;stop step-&gt;predict[label=repeat] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G init init predict predict init&#45;&gt;predict loss loss predict&#45;&gt;loss gradient gradient loss&#45;&gt;gradient step step gradient&#45;&gt;step step&#45;&gt;predict repeat stop stop step&#45;&gt;stop def f(x): return x**2 . plot_function(f, &#39;x&#39;, &#39;x**2&#39;) . plot_function(f, &#39;x&#39;, &#39;x**2&#39;) plt.scatter(-1.5, f(-1.5), color=&#39;red&#39;); . Calculating Gradients . x1 = -1. . xt = tensor(x1).requires_grad_() xt . tensor(-1., requires_grad=True) . yt = f(xt) yt.backward() yt . tensor(1., grad_fn=&lt;PowBackward0&gt;) . xt.grad . tensor(-2.) . def grad(x): return (x * xt.grad.item()) - yt.item() x = torch.linspace(-2,2) fig,ax = plt.subplots(figsize=(6,4)) ax.plot(x, f(x)) ax.plot(x, grad(x)) plt.scatter(x1, f(x1), color=&#39;red&#39;); plt.ylim(-.1, 4.1) . (-0.1, 4.1) . 3D gradients . xt = tensor([3.,4.,10.]).requires_grad_() xt . tensor([ 3., 4., 10.], requires_grad=True) . def f(x): return (x**2).sum() yt = f(xt) yt . tensor(125., grad_fn=&lt;SumBackward0&gt;) . yt.backward() xt.grad . tensor([ 6., 8., 20.]) . Stepping With a Learning Rate . An End-to-End SGD Example . time = torch.arange(0,20).float(); time . tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.]) . speed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1 plt.scatter(time,speed); . $f(x, a, b, c) = ax^2 + bx + c$ . def f(t, params): a,b,c = params return a*(t**2) + (b*t) + c . $mse(predictions, targets) = sqrt{ frac{ sum(predictions - targets)^2}{n}} $ . def mse(preds, targets): return ((preds-targets)**2).mean().sqrt() . Step 1: Initialize the parameters . params = torch.randn(3).requires_grad_() . Step 2: Calculate the predictions . def show_preds(preds, ax=None): if ax is None: ax=plt.subplots()[1] ax.scatter(time, speed) ax.scatter(time, to_np(preds), color=&#39;red&#39;) ax.set_xlabel(&#39;time&#39;) ax.set_ylabel(&#39;predictions&#39;) ax.set_ylim(-300,100) . preds = f(time, params) . show_preds(preds) . Step 3: Calculate the loss . loss = mse(preds, speed) loss . tensor(141.6118, grad_fn=&lt;SqrtBackward&gt;) . Step 4: Calculate the gradients . loss.backward() params.grad . tensor([-164.9038, -10.5370, -0.7816]) . params.grad * 1e-5 . tensor([-1.6490e-03, -1.0537e-04, -7.8159e-06]) . params . tensor([-0.7244, 0.3629, 1.9200], requires_grad=True) . Step 5: Step the weights. . lr = 1e-3 params.data -= lr * params.grad.data params.grad = None . preds = f(time,params) mse(preds, speed) . tensor(114.4197, grad_fn=&lt;SqrtBackward&gt;) . show_preds(preds) . def apply_step(params, prn=True): preds = f(time, params) loss = mse(preds, speed) loss.backward() params.data -= lr * params.grad.data params.grad = None if prn: print(loss.item()) return preds . Step 6: Repeat the process . for i in range(10): apply_step(params) . 114.41970825195312 87.83831024169922 62.552642822265625 40.56702423095703 27.578344345092773 25.898353576660156 25.897171020507812 25.897003173828125 25.89684295654297 25.896682739257812 . _,axs = plt.subplots(1,5,figsize=(15,3)) for ax in axs: show_preds(apply_step(params, False), ax) plt.tight_layout() . _,axs = plt.subplots(1,5,figsize=(15,3)) for ax in axs: show_preds(apply_step(params, False), ax) plt.tight_layout() . Step 7: stop . Summarizing Gradient Descent . gv(&#39;&#39;&#39; init-&gt;predict-&gt;loss-&gt;gradient-&gt;step-&gt;stop step-&gt;predict[label=repeat] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G init init predict predict init&#45;&gt;predict loss loss predict&#45;&gt;loss gradient gradient loss&#45;&gt;gradient step step gradient&#45;&gt;step step&#45;&gt;predict repeat stop stop step&#45;&gt;stop The MNIST Loss Function . train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28) . train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1) train_x.shape, train_y.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . dset = list(zip(train_x,train_y)) x,y = dset[0] len(dset), x.shape, y.shape . (12396, torch.Size([784]), torch.Size([1])) . valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28) valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) . def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() . weights = init_params((28*28,1)) . bias = init_params(1) . (train_x[0]*weights.T).sum() + bias . tensor([3.7602], grad_fn=&lt;AddBackward0&gt;) . def linear1(xb): return xb@weights + bias preds = linear1(train_x) preds . tensor([[ 3.7602], [10.0223], [15.1395], ..., [ 4.7646], [ 1.8502], [ 3.3399]], grad_fn=&lt;AddBackward0&gt;) . corrects = (preds&gt;0.0).float() == train_y corrects . tensor([[ True], [ True], [ True], ..., [False], [False], [False]]) . corrects.float().mean().item() . 0.4820910096168518 . weights[0] *= 1.0001 . weights.shape . torch.Size([784, 1]) . preds = linear1(train_x) ((preds&gt;0.0).float() == train_y).float().mean().item() . 0.4820910096168518 . trgts = tensor([1,0,1]) prds = tensor([0.9, 0.4, 0.2]) . def mnist_loss(predictions, targets): return torch.where(targets==1, 1-predictions, predictions).mean() . torch.where(trgts==1, 1-prds, prds) . tensor([0.1000, 0.4000, 0.8000]) . mnist_loss(prds,trgts) . tensor(0.4333) . mnist_loss(tensor([0.9, 0.4, 0.8]),trgts) . tensor(0.2333) . Sigmoid . def sigmoid(x): return 1/(1+torch.exp(-x)) . plot_function(torch.sigmoid, title=&#39;Sigmoid&#39;, min=-6, max=6) . def mnist_loss(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1-predictions, predictions).mean() . mnist_loss(tensor([1., 0., 1.]), tensor([1.,0.,1.])) . tensor(0.3460) . SGD and Mini-Batches . coll = range(15) dl = DataLoader(coll, batch_size=5, shuffle=True) list(dl) . [tensor([ 3, 12, 8, 10, 2]), tensor([ 9, 4, 7, 14, 5]), tensor([ 1, 13, 0, 6, 11])] . ds = L(enumerate(string.ascii_lowercase)) ds . (#26) [(0, &#39;a&#39;),(1, &#39;b&#39;),(2, &#39;c&#39;),(3, &#39;d&#39;),(4, &#39;e&#39;),(5, &#39;f&#39;),(6, &#39;g&#39;),(7, &#39;h&#39;),(8, &#39;i&#39;),(9, &#39;j&#39;)...] . dl = DataLoader(ds, batch_size=6, shuffle=True) list(dl) . [(tensor([17, 18, 10, 22, 8, 14]), (&#39;r&#39;, &#39;s&#39;, &#39;k&#39;, &#39;w&#39;, &#39;i&#39;, &#39;o&#39;)), (tensor([20, 15, 9, 13, 21, 12]), (&#39;u&#39;, &#39;p&#39;, &#39;j&#39;, &#39;n&#39;, &#39;v&#39;, &#39;m&#39;)), (tensor([ 7, 25, 6, 5, 11, 23]), (&#39;h&#39;, &#39;z&#39;, &#39;g&#39;, &#39;f&#39;, &#39;l&#39;, &#39;x&#39;)), (tensor([ 1, 3, 0, 24, 19, 16]), (&#39;b&#39;, &#39;d&#39;, &#39;a&#39;, &#39;y&#39;, &#39;t&#39;, &#39;q&#39;)), (tensor([2, 4]), (&#39;c&#39;, &#39;e&#39;))] . Putting It All Together . weights = init_params((28*28,1)) bias = init_params(1) . dl = DataLoader(dset, batch_size=256) xb,yb = first(dl) xb.shape,yb.shape . (torch.Size([256, 784]), torch.Size([256, 1])) . valid_dl = DataLoader(valid_dset, batch_size=256) . batch = train_x[:4] batch.shape . torch.Size([4, 784]) . preds = linear1(batch) preds . tensor([[ 9.7460], [14.8540], [ 6.6535], [11.8257]], grad_fn=&lt;AddBackward0&gt;) . loss = mnist_loss(preds, train_y[:4]) loss . tensor(0.0003, grad_fn=&lt;MeanBackward0&gt;) . loss.backward() weights.grad.shape,weights.grad.mean(),bias.grad . (torch.Size([784, 1]), tensor(-4.8543e-05), tensor([-0.0003])) . def calc_grad(xb, yb, model): preds = model(xb) loss = mnist_loss(preds, yb) loss.backward() . calc_grad(batch, train_y[:4], linear1) weights.grad.mean(),bias.grad . (tensor(-9.7085e-05), tensor([-0.0007])) . calc_grad(batch, train_y[:4], linear1) weights.grad.mean(),bias.grad . (tensor(-0.0001), tensor([-0.0010])) . weights.grad.zero_() bias.grad.zero_(); . def train_epoch(model, lr, params): for xb,yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() . (preds&gt;0.0).float() == train_y[:4] . tensor([[True], [True], [True], [True]]) . def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() . batch_accuracy(linear1(batch), train_y[:4]) . tensor(1.) . def validate_epoch(model): accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl] return round(torch.stack(accs).mean().item(), 4) . validate_epoch(linear1) . 0.6554 . lr = 1. params = weights,bias train_epoch(linear1, lr, params) validate_epoch(linear1) . 0.7372 . for i in range(20): train_epoch(linear1, lr, params) print(validate_epoch(linear1), end=&#39; &#39;) . 0.8886 0.932 0.9457 0.954 0.9584 0.9598 0.9623 0.9643 0.9648 0.9662 0.9672 0.9677 0.9682 0.9696 0.9701 0.9701 0.9706 0.9716 0.9721 0.9721 . Creating an Optimizer . linear_model = nn.Linear(28*28,1) . w,b = linear_model.parameters() w.shape,b.shape . (torch.Size([1, 784]), torch.Size([1])) . class BasicOptim: def __init__(self,params,lr): self.params,self.lr = list(params),lr def step(self, *args, **kwargs): for p in self.params: p.data -= p.grad.data * self.lr def zero_grad(self, *args, **kwargs): for p in self.params: p.grad = None . opt = BasicOptim(linear_model.parameters(), lr) . def train_epoch(model): for xb,yb in dl: calc_grad(xb, yb, model) opt.step() opt.zero_grad() . validate_epoch(linear_model) . 0.4562 . def train_model(model, epochs): for i in range(epochs): train_epoch(model) print(validate_epoch(model), end=&#39; &#39;) . train_model(linear_model, 20) . 0.4932 0.7354 0.8564 0.9179 0.9345 0.9487 0.957 0.9638 0.9658 0.9672 0.9692 0.9716 0.9746 0.9746 0.976 0.9765 0.9775 0.9775 0.978 0.9785 . linear_model = nn.Linear(28*28,1) opt = SGD(linear_model.parameters(), lr) train_model(linear_model, 20) . 0.4932 0.8091 0.8481 0.916 0.9336 0.9487 0.956 0.9638 0.9663 0.9682 0.9692 0.9721 0.9736 0.9751 0.976 0.977 0.9775 0.978 0.978 0.9785 . dls = DataLoaders(dl, valid_dl) . learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . learn.fit(20, lr=lr) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.637234 | 0.503652 | 0.495584 | 00:00 | . 1 | 0.622813 | 0.099536 | 0.934740 | 00:00 | . 2 | 0.224453 | 0.229068 | 0.785574 | 00:00 | . 3 | 0.096522 | 0.119550 | 0.899411 | 00:00 | . 4 | 0.049351 | 0.083758 | 0.929343 | 00:00 | . 5 | 0.031013 | 0.065843 | 0.941119 | 00:00 | . 6 | 0.023518 | 0.055016 | 0.954367 | 00:00 | . 7 | 0.020242 | 0.047905 | 0.961236 | 00:00 | . 8 | 0.018628 | 0.042962 | 0.964671 | 00:00 | . 9 | 0.017691 | 0.039352 | 0.967125 | 00:00 | . 10 | 0.017050 | 0.036603 | 0.968597 | 00:00 | . 11 | 0.016553 | 0.034435 | 0.971541 | 00:00 | . 12 | 0.016141 | 0.032674 | 0.972522 | 00:00 | . 13 | 0.015789 | 0.031212 | 0.973994 | 00:00 | . 14 | 0.015485 | 0.029979 | 0.975957 | 00:00 | . 15 | 0.015220 | 0.028927 | 0.976448 | 00:00 | . 16 | 0.014989 | 0.028021 | 0.977429 | 00:00 | . 17 | 0.014785 | 0.027235 | 0.977429 | 00:00 | . 18 | 0.014602 | 0.026546 | 0.977920 | 00:00 | . 19 | 0.014437 | 0.025939 | 0.978410 | 00:00 | . Adding a Nonlinearity . def simple_net(xb): res = xb@w1 + b1 res = res.max(tensor(0.0)) res = res@w2 + b2 return res . w1 = init_params((28*28,30)) b1 = init_params(30) w2 = init_params((30,1)) b2 = init_params(1) . plot_function(F.relu) . simple_net = nn.Sequential( nn.Linear(28*28,30), nn.ReLU(), nn.Linear(30,1) ) . learn = Learner(dls, simple_net, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . learn.fit(40, 0.1) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.316475 | 0.418682 | 0.504416 | 00:00 | . 1 | 0.149121 | 0.231948 | 0.799313 | 00:00 | . 2 | 0.082135 | 0.115575 | 0.915604 | 00:00 | . 3 | 0.053724 | 0.077822 | 0.940137 | 00:00 | . 4 | 0.040625 | 0.060725 | 0.955839 | 00:00 | . 5 | 0.033974 | 0.051135 | 0.964181 | 00:00 | . 6 | 0.030173 | 0.045101 | 0.965653 | 00:00 | . 7 | 0.027711 | 0.040987 | 0.965653 | 00:00 | . 8 | 0.025942 | 0.037986 | 0.968106 | 00:00 | . 9 | 0.024574 | 0.035688 | 0.971050 | 00:00 | . 10 | 0.023464 | 0.033859 | 0.972522 | 00:00 | . 11 | 0.022537 | 0.032356 | 0.973013 | 00:00 | . 12 | 0.021745 | 0.031094 | 0.973503 | 00:00 | . 13 | 0.021058 | 0.030013 | 0.974975 | 00:00 | . 14 | 0.020455 | 0.029072 | 0.975466 | 00:00 | . 15 | 0.019920 | 0.028244 | 0.976938 | 00:00 | . 16 | 0.019441 | 0.027508 | 0.977920 | 00:00 | . 17 | 0.019008 | 0.026849 | 0.977920 | 00:00 | . 18 | 0.018614 | 0.026254 | 0.977920 | 00:00 | . 19 | 0.018253 | 0.025716 | 0.978901 | 00:00 | . 20 | 0.017922 | 0.025225 | 0.978901 | 00:00 | . 21 | 0.017615 | 0.024774 | 0.979392 | 00:00 | . 22 | 0.017330 | 0.024360 | 0.979882 | 00:00 | . 23 | 0.017064 | 0.023978 | 0.980373 | 00:00 | . 24 | 0.016814 | 0.023624 | 0.980864 | 00:00 | . 25 | 0.016579 | 0.023295 | 0.980864 | 00:00 | . 26 | 0.016357 | 0.022990 | 0.981845 | 00:00 | . 27 | 0.016148 | 0.022705 | 0.981845 | 00:00 | . 28 | 0.015949 | 0.022439 | 0.982336 | 00:00 | . 29 | 0.015761 | 0.022191 | 0.982336 | 00:00 | . 30 | 0.015581 | 0.021958 | 0.982336 | 00:00 | . 31 | 0.015410 | 0.021740 | 0.983317 | 00:00 | . 32 | 0.015247 | 0.021534 | 0.983808 | 00:00 | . 33 | 0.015091 | 0.021340 | 0.983808 | 00:00 | . 34 | 0.014941 | 0.021157 | 0.983808 | 00:00 | . 35 | 0.014797 | 0.020985 | 0.983317 | 00:00 | . 36 | 0.014659 | 0.020822 | 0.983317 | 00:00 | . 37 | 0.014525 | 0.020667 | 0.983317 | 00:00 | . 38 | 0.014396 | 0.020520 | 0.983317 | 00:00 | . 39 | 0.014272 | 0.020378 | 0.983317 | 00:00 | . plt.plot(L(learn.recorder.values).itemgot(2)); . learn.recorder.values[-1][2] . 0.983316957950592 . Going Deeper . dls = ImageDataLoaders.from_folder(path) learn = cnn_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(1, 0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.111226 | 0.045557 | 0.993621 | 00:19 | . Jargon Recap . Questionnaire . How is a grayscale image represented on a computer? How about a color image? | How are the files and folders in the MNIST_SAMPLE dataset structured? Why? | Explain how the &quot;pixel similarity&quot; approach to classifying digits works. | What is a list comprehension? Create one now that selects odd numbers from a list and doubles them. | What is a &quot;rank-3 tensor&quot;? | What is the difference between tensor rank and shape? How do you get the rank from the shape? | What are RMSE and L1 norm? | How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop? | Create a 3×3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers. | What is broadcasting? | Are metrics generally calculated using the training set, or the validation set? Why? | What is SGD? | Why does SGD use mini-batches? | What are the seven steps in SGD for machine learning? | How do we initialize the weights in a model? | What is &quot;loss&quot;? | Why can&#39;t we always use a high learning rate? | What is a &quot;gradient&quot;? | Do you need to know how to calculate gradients yourself? | Why can&#39;t we use accuracy as a loss function? | Draw the sigmoid function. What is special about its shape? | What is the difference between a loss function and a metric? | What is the function to calculate new weights using a learning rate? | What does the DataLoader class do? | Write pseudocode showing the basic steps taken in each epoch for SGD. | Create a function that, if passed two arguments [1,2,3,4] and &#39;abcd&#39;, returns [(1, &#39;a&#39;), (2, &#39;b&#39;), (3, &#39;c&#39;), (4, &#39;d&#39;)]. What is special about that output data structure? | What does view do in PyTorch? | What are the &quot;bias&quot; parameters in a neural network? Why do we need them? | What does the @ operator do in Python? | What does the backward method do? | Why do we have to zero the gradients? | What information do we have to pass to Learner? | Show Python or pseudocode for the basic steps of a training loop. | What is &quot;ReLU&quot;? Draw a plot of it for values from -2 to +2. | What is an &quot;activation function&quot;? | What&#39;s the difference between F.relu and nn.ReLU? | The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more? | Further Research . Create your own implementation of Learner from scratch, based on the training loop shown in this chapter. | Complete all the steps in this chapter using the full MNIST datasets (that is, for all digits, not just 3s and 7s). This is a significant project and will take you quite a bit of time to complete! You&#39;ll need to do some of your own research to figure out how to overcome some obstacles you&#39;ll meet on the way. |",
            "url": "https://chrismilleruk.github.io/fastpages/2021/01/07/fastai-04_mnist_basics.html",
            "relUrl": "/2021/01/07/fastai-04_mnist_basics.html",
            "date": " • Jan 7, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "From Model to Production",
            "content": "The Practice of Deep Learning . Starting Your Project . The State of Deep Learning . Computer vision . Text (natural language processing) . Combining text and images . Tabular data . Recommendation systems . Other data types . The Drivetrain Approach . Gathering Data . clean . To download images with Bing Image Search, sign up at Microsoft Azure for a free account. You will be given a key, which you can copy and enter in a cell as follows (replacing &#39;XXX&#39; with your key and executing it): . search_images_ddg . &lt;function fastbook.search_images_ddg(term, max_images=200)&gt; . search_images_bing . &lt;function fastbook.search_images_bing(key, term, min_sz=128, max_images=150)&gt; . results = search_images_bing(key, &#39;grizzly bear&#39;) ims = results.attrgot(&#39;contentUrl&#39;) len(ims) . 150 . ims . (#150) [&#39;https://images.gearjunkie.com/uploads/2015/07/Grizzly-Bear.jpg&#39;,&#39;http://someinterestingfacts.net/wp-content/uploads/2016/07/Canadian-Grizzly-Bear.jpg&#39;,&#39;http://www.pbs.org/wnet/nature/files/2018/07/Bear133.jpg&#39;,&#39;http://wildlifearticles.co.uk/wp-content/uploads/2015/10/grizzly-bear3.jpg&#39;,&#39;https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/GrizzlyBearJeanBeaufort.jpg/1200px-GrizzlyBearJeanBeaufort.jpg&#39;,&#39;https://gohunt-assets-us-west-2.s3.amazonaws.com/wyoming-grizzly-bear-og_0.jpg&#39;,&#39;https://upload.wikimedia.org/wikipedia/commons/e/e2/Grizzlybear55.jpg&#39;,&#39;https://i0.wp.com/www.commonsenseevaluation.com/wp-content/uploads/2013/08/Bear.jpg&#39;,&#39;https://www.tsln.com/wp-content/uploads/2018/10/bears-tsln-101318-3-1240x826.jpg&#39;,&#39;https://d3d0lqu00lnqvz.cloudfront.net/media/media/897b2e5d-6d4c-40fa-bbe8-6829455747e2.jpg&#39;...] . dest = &#39;../images/grizzly.jpg&#39; download_url(ims[0], dest) . im = Image.open(dest) im.to_thumb(128,128) . bear_types = &#39;grizzly&#39;,&#39;black&#39;,&#39;teddy&#39; path = Path(&#39;bears&#39;) . if not path.exists(): path.mkdir() for o in bear_types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{o} bear&#39;) download_images(dest, urls=results.attrgot(&#39;contentUrl&#39;)) . fns = get_image_files(path) fns . (#436) [Path(&#39;bears/grizzly/00000007.jpg&#39;),Path(&#39;bears/grizzly/00000002.jpg&#39;),Path(&#39;bears/grizzly/00000001.jpg&#39;),Path(&#39;bears/grizzly/00000004.jpg&#39;),Path(&#39;bears/grizzly/00000006.jpg&#39;),Path(&#39;bears/grizzly/00000008.jpg&#39;),Path(&#39;bears/grizzly/00000009.jpg&#39;),Path(&#39;bears/grizzly/00000003.jpg&#39;),Path(&#39;bears/grizzly/00000014.jpg&#39;),Path(&#39;bears/grizzly/00000012.jpg&#39;)...] . failed = verify_images(fns) failed . (#0) [] . failed.map(Path.unlink); . Sidebar: Getting Help in Jupyter Notebooks . End sidebar . From Data to DataLoaders . bears = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . doc(parent_label) . dls = bears.dataloaders(path) . dls.valid.show_batch(max_n=12, nrows=2) . bears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish)) dls = bears.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . bears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode=&#39;zeros&#39;)) dls = bears.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . bears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3)) dls = bears.dataloaders(path) dls.train.show_batch(max_n=4, nrows=1, unique=True) . Data Augmentation . bears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2)) dls = bears.dataloaders(path) dls.train.show_batch(max_n=8, nrows=2, unique=True) . Training Your Model, and Using It to Clean Your Data . bears = bears.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = bears.dataloaders(path) . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.248772 | 0.102475 | 0.034483 | 00:10 | . epoch train_loss valid_loss error_rate time . 0 | 0.093664 | 0.066300 | 0.022989 | 00:11 | . 1 | 0.067296 | 0.090083 | 0.022989 | 00:10 | . 2 | 0.047179 | 0.104809 | 0.022989 | 00:10 | . 3 | 0.037067 | 0.096211 | 0.034483 | 00:12 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . interp.plot_top_losses(5, nrows=1) . cleaner = ImageClassifierCleaner(learn) cleaner . Turning Your Model into an Online Application . Using the Model for Inference . learn.export() . path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . learn_inf = load_learner(path/&#39;export.pkl&#39;) . learn_inf.predict(&#39;../images/grizzly.jpg&#39;) . (&#39;grizzly&#39;, TensorImage(1), TensorImage([3.2993e-07, 1.0000e+00, 1.7181e-07])) . learn_inf.dls.vocab . [&#39;black&#39;, &#39;grizzly&#39;, &#39;teddy&#39;] . Creating a Notebook App from the Model . btn_upload = widgets.FileUpload() btn_upload . img = PILImage.create(btn_upload.data[-1]) . out_pl = widgets.Output() out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) out_pl . pred,pred_idx,probs = learn_inf.predict(img) . lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; lbl_pred . btn_run = widgets.Button(description=&#39;Classify&#39;) btn_run . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . VBox([widgets.Label(&#39;Select your bear!&#39;), btn_upload, btn_run, out_pl, lbl_pred]) . Turning Your Notebook into a Real App . Deploying your app . How to Avoid Disaster . Unforeseen Consequences and Feedback Loops . Get Writing! . Questionnaire . Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data. | Where do text models currently have a major deficiency? | What are possible negative societal implications of text generation models? | In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process? | What kind of tabular data is deep learning particularly good at? | What&#39;s a key downside of directly using a deep learning model for recommendation systems? | What are the steps of the Drivetrain Approach? | How do the steps of the Drivetrain Approach map to a recommendation system? | Create an image recognition model using data you curate, and deploy it on the web. | What is DataLoaders? | What four things do we need to tell fastai to create DataLoaders? | What does the splitter parameter to DataBlock do? | How do we ensure a random split always gives the same validation set? | What letters are often used to signify the independent and dependent variables? | What&#39;s the difference between the crop, pad, and squish resize approaches? When might you choose one over the others? | What is data augmentation? Why is it needed? | What is the difference between item_tfms and batch_tfms? | What is a confusion matrix? | What does export save? | What is it called when we use a model for getting predictions, instead of training? | What are IPython widgets? | When might you want to use CPU for deployment? When might GPU be better? | What are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC? | What are three examples of problems that could occur when rolling out a bear warning system in practice? | What is &quot;out-of-domain data&quot;? | What is &quot;domain shift&quot;? | What are the three steps in the deployment process? | Further Research . Consider how the Drivetrain Approach maps to a project or problem you&#39;re interested in. | When might it be best to avoid certain types of data augmentation? | For a project you&#39;re interested in applying deep learning to, consider the thought experiment &quot;What would happen if it went really, really well?&quot; | Start a blog, and write your first blog post. For instance, write about what you think deep learning might be useful for in a domain you&#39;re interested in. |",
            "url": "https://chrismilleruk.github.io/fastpages/2021/01/06/fastai-02_production.html",
            "relUrl": "/2021/01/06/fastai-02_production.html",
            "date": " • Jan 6, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Your Deep Learning Journey",
            "content": "Deep Learning Is for Everyone . Neural Networks: A Brief History . Who We Are . How to Learn Deep Learning . Your Projects and Your Mindset . The Software: PyTorch, fastai, and Jupyter . Your First Model . Getting a GPU Deep Learning Server . Running Your First Notebook . from fastai.vision.all import * path = untar_data(URLs.PETS)/&#39;images&#39; def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func( path, get_image_files(path), valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224)) learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.161300 | 0.021319 | 0.009472 | 00:33 | . epoch train_loss valid_loss error_rate time . 0 | 0.069097 | 0.040982 | 0.009472 | 00:43 | . Sidebar: This Book Was Written in Jupyter Notebooks . 1+1 . 2 . img = PILImage.create(image_cat()) img.to_thumb(192) . End sidebar . uploader = widgets.FileUpload() uploader . img = PILImage.create(uploader.data[0]) is_cat,_,probs = learn.predict(img) print(f&quot;Is this a cat?: {is_cat}.&quot;) print(f&quot;Probability it&#39;s a cat: {probs[1].item():.6f}&quot;) . NameError Traceback (most recent call last) &lt;ipython-input-13-37db1363ed76&gt; in &lt;module&gt; 1 img = PILImage.create(uploader.data[0]) -&gt; 2 is_cat,_,probs = learn.predict(img) 3 print(f&#34;Is this a cat?: {is_cat}.&#34;) 4 print(f&#34;Probability it&#39;s a cat: {probs[1].item():.6f}&#34;) NameError: name &#39;learn&#39; is not defined . What Is Machine Learning? . gv(&#39;&#39;&#39;program[shape=box3d width=1 height=0.7] inputs-&gt;program-&gt;results&#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G program program results results program&#45;&gt;results inputs inputs inputs&#45;&gt;program gv(&#39;&#39;&#39;model[shape=box3d width=1 height=0.7] inputs-&gt;model-&gt;results; weights-&gt;model&#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G model model results results model&#45;&gt;results inputs inputs inputs&#45;&gt;model weights weights weights&#45;&gt;model gv(&#39;&#39;&#39;ordering=in model[shape=box3d width=1 height=0.7] inputs-&gt;model-&gt;results; weights-&gt;model; results-&gt;performance performance-&gt;weights[constraint=false label=update]&#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G model model results results model&#45;&gt;results inputs inputs inputs&#45;&gt;model performance performance results&#45;&gt;performance weights weights weights&#45;&gt;model performance&#45;&gt;weights update gv(&#39;&#39;&#39;model[shape=box3d width=1 height=0.7] inputs-&gt;model-&gt;results&#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G model model results results model&#45;&gt;results inputs inputs inputs&#45;&gt;model What Is a Neural Network? . A Bit of Deep Learning Jargon . gv(&#39;&#39;&#39;ordering=in model[shape=box3d width=1 height=0.7 label=architecture] inputs-&gt;model-&gt;predictions; parameters-&gt;model; labels-&gt;loss; predictions-&gt;loss loss-&gt;parameters[constraint=false label=update]&#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G model architecture predictions predictions model&#45;&gt;predictions inputs inputs inputs&#45;&gt;model loss loss predictions&#45;&gt;loss parameters parameters parameters&#45;&gt;model labels labels labels&#45;&gt;loss loss&#45;&gt;parameters update Limitations Inherent To Machine Learning . From this picture we can now see some fundamental things about training a deep learning model: . A model cannot be created without data. | A model can only learn to operate on the patterns seen in the input data used to train it. | This learning approach only creates predictions, not recommended actions. | It&#39;s not enough to just have examples of input data; we need labels for that data too (e.g., pictures of dogs and cats aren&#39;t enough to train a model; we need a label for each one, saying which ones are dogs, and which are cats). | . Generally speaking, we&#39;ve seen that most organizations that say they don&#39;t have enough data, actually mean they don&#39;t have enough labeled data. If any organization is interested in doing something in practice with a model, then presumably they have some inputs they plan to run their model against. And presumably they&#39;ve been doing that some other way for a while (e.g., manually, or with some heuristic program), so they have data from those processes! For instance, a radiology practice will almost certainly have an archive of medical scans (since they need to be able to check how their patients are progressing over time), but those scans may not have structured labels containing a list of diagnoses or interventions (since radiologists generally create free-text natural language reports, not structured data). We&#39;ll be discussing labeling approaches a lot in this book, because it&#39;s such an important issue in practice. . Since these kinds of machine learning models can only make predictions (i.e., attempt to replicate labels), this can result in a significant gap between organizational goals and model capabilities. For instance, in this book you&#39;ll learn how to create a recommendation system that can predict what products a user might purchase. This is often used in e-commerce, such as to customize products shown on a home page by showing the highest-ranked items. But such a model is generally created by looking at a user and their buying history (inputs) and what they went on to buy or look at (labels), which means that the model is likely to tell you about products the user already has or already knows about, rather than new products that they are most likely to be interested in hearing about. That&#39;s very different to what, say, an expert at your local bookseller might do, where they ask questions to figure out your taste, and then tell you about authors or series that you&#39;ve never heard of before. . How Our Image Recognizer Works . What Our Image Recognizer Learned . Image Recognizers Can Tackle Non-Image Tasks . Jargon Recap . Deep Learning Is Not Just for Image Classification . path = untar_data(URLs.CAMVID_TINY) dls = SegmentationDataLoaders.from_label_func( path, bs=8, fnames = get_image_files(path/&quot;images&quot;), label_func = lambda o: path/&#39;labels&#39;/f&#39;{o.stem}_P{o.suffix}&#39;, codes = np.loadtxt(path/&#39;codes.txt&#39;, dtype=str) ) learn = unet_learner(dls, resnet34) learn.fine_tune(8) . epoch train_loss valid_loss time . 0 | 2.987978 | 2.480352 | 00:21 | . epoch train_loss valid_loss time . 0 | 2.158624 | 1.743689 | 00:23 | . 1 | 1.767877 | 1.964173 | 00:26 | . 2 | 1.643144 | 1.283908 | 00:25 | . 3 | 1.472303 | 1.108046 | 00:25 | . 4 | 1.320544 | 0.973587 | 00:25 | . 5 | 1.183503 | 0.858053 | 00:26 | . 6 | 1.073220 | 0.822954 | 00:26 | . 7 | 0.986720 | 0.817717 | 00:25 | . learn.show_results(max_n=6, figsize=(7,8)) . from fastai.text.all import * dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid=&#39;test&#39;) learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy) learn.fine_tune(4, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.809147 | 01:06 | . KeyboardInterrupt Traceback (most recent call last) &lt;ipython-input-17-5ab79cd5e866&gt; in &lt;module&gt; 3 dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid=&#39;test&#39;) 4 learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy) -&gt; 5 learn.fine_tune(4, 1e-2) /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/callback/schedule.py in fine_tune(self, epochs, base_lr, freeze_epochs, lr_mult, pct_start, div, **kwargs) 155 &#34;Fine tune with `freeze` for `freeze_epochs` then with `unfreeze` from `epochs` using discriminative LR&#34; 156 self.freeze() --&gt; 157 self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs) 158 base_lr /= 2 159 self.unfreeze() /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/callback/schedule.py in fit_one_cycle(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt) 110 scheds = {&#39;lr&#39;: combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final), 111 &#39;mom&#39;: combined_cos(pct_start, *(self.moms if moms is None else moms))} --&gt; 112 self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd) 113 114 # Cell /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/learner.py in fit(self, n_epoch, lr, wd, cbs, reset_opt) 203 self.opt.set_hypers(lr=self.lr if lr is None else lr) 204 self.n_epoch = n_epoch --&gt; 205 self._with_events(self._do_fit, &#39;fit&#39;, CancelFitException, self._end_cleanup) 206 207 def _end_cleanup(self): self.dl,self.xb,self.yb,self.pred,self.loss = None,(None,),(None,),None,None /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final) 152 153 def _with_events(self, f, event_type, ex, final=noop): --&gt; 154 try: self(f&#39;before_{event_type}&#39;) ;f() 155 except ex: self(f&#39;after_cancel_{event_type}&#39;) 156 finally: self(f&#39;after_{event_type}&#39;) ;final() /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/learner.py in _do_fit(self) 194 for epoch in range(self.n_epoch): 195 self.epoch=epoch --&gt; 196 self._with_events(self._do_epoch, &#39;epoch&#39;, CancelEpochException) 197 198 def fit(self, n_epoch, lr=None, wd=None, cbs=None, reset_opt=False): /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final) 152 153 def _with_events(self, f, event_type, ex, final=noop): --&gt; 154 try: self(f&#39;before_{event_type}&#39;) ;f() 155 except ex: self(f&#39;after_cancel_{event_type}&#39;) 156 finally: self(f&#39;after_{event_type}&#39;) ;final() /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/learner.py in _do_epoch(self) 188 189 def _do_epoch(self): --&gt; 190 self._do_epoch_train() 191 self._do_epoch_validate() 192 /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/learner.py in _do_epoch_train(self) 180 def _do_epoch_train(self): 181 self.dl = self.dls.train --&gt; 182 self._with_events(self.all_batches, &#39;train&#39;, CancelTrainException) 183 184 def _do_epoch_validate(self, ds_idx=1, dl=None): /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final) 152 153 def _with_events(self, f, event_type, ex, final=noop): --&gt; 154 try: self(f&#39;before_{event_type}&#39;) ;f() 155 except ex: self(f&#39;after_cancel_{event_type}&#39;) 156 finally: self(f&#39;after_{event_type}&#39;) ;final() /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/learner.py in all_batches(self) 158 def all_batches(self): 159 self.n_iter = len(self.dl) --&gt; 160 for o in enumerate(self.dl): self.one_batch(*o) 161 162 def _do_one_batch(self): /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/learner.py in one_batch(self, i, b) 176 self.iter = i 177 self._split(b) --&gt; 178 self._with_events(self._do_one_batch, &#39;batch&#39;, CancelBatchException) 179 180 def _do_epoch_train(self): /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final) 152 153 def _with_events(self, f, event_type, ex, final=noop): --&gt; 154 try: self(f&#39;before_{event_type}&#39;) ;f() 155 except ex: self(f&#39;after_cancel_{event_type}&#39;) 156 finally: self(f&#39;after_{event_type}&#39;) ;final() /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/learner.py in _do_one_batch(self) 161 162 def _do_one_batch(self): --&gt; 163 self.pred = self.model(*self.xb) 164 self(&#39;after_pred&#39;) 165 if len(self.yb): self.loss = self.loss_func(self.pred, *self.yb) /opt/conda/envs/fastai/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 725 result = self._slow_forward(*input, **kwargs) 726 else: --&gt; 727 result = self.forward(*input, **kwargs) 728 for hook in itertools.chain( 729 _global_forward_hooks.values(), /opt/conda/envs/fastai/lib/python3.8/site-packages/torch/nn/modules/container.py in forward(self, input) 115 def forward(self, input): 116 for module in self: --&gt; 117 input = module(input) 118 return input 119 /opt/conda/envs/fastai/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 725 result = self._slow_forward(*input, **kwargs) 726 else: --&gt; 727 result = self.forward(*input, **kwargs) 728 for hook in itertools.chain( 729 _global_forward_hooks.values(), /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/text/models/core.py in forward(self, input) 79 #Note: this expects that sequence really begins on a round multiple of bptt 80 real_bs = (input[:,i] != self.pad_idx).long().sum() &gt; 81 o = self.module(input[:real_bs,i: min(i+self.bptt, sl)]) 82 if self.max_len is None or sl-i &lt;= self.max_len: 83 outs.append(o) /opt/conda/envs/fastai/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 725 result = self._slow_forward(*input, **kwargs) 726 else: --&gt; 727 result = self.forward(*input, **kwargs) 728 for hook in itertools.chain( 729 _global_forward_hooks.values(), /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/text/models/awdlstm.py in forward(self, inp, from_embeds) 104 new_hidden = [] 105 for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)): --&gt; 106 output, new_h = rnn(output, self.hidden[l]) 107 new_hidden.append(new_h) 108 if l != self.n_layers - 1: output = hid_dp(output) /opt/conda/envs/fastai/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 725 result = self._slow_forward(*input, **kwargs) 726 else: --&gt; 727 result = self.forward(*input, **kwargs) 728 for hook in itertools.chain( 729 _global_forward_hooks.values(), /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/text/models/awdlstm.py in forward(self, *args) 51 # To avoid the warning that comes because the weights aren&#39;t flattened. 52 warnings.simplefilter(&#34;ignore&#34;, category=UserWarning) &gt; 53 return self.module(*args) 54 55 def reset(self): /opt/conda/envs/fastai/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 725 result = self._slow_forward(*input, **kwargs) 726 else: --&gt; 727 result = self.forward(*input, **kwargs) 728 for hook in itertools.chain( 729 _global_forward_hooks.values(), /opt/conda/envs/fastai/lib/python3.8/site-packages/torch/nn/modules/rnn.py in forward(self, input, hx) 579 self.check_forward_args(input, hx, batch_sizes) 580 if batch_sizes is None: --&gt; 581 result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers, 582 self.dropout, self.training, self.bidirectional, self.batch_first) 583 else: /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/torch_core.py in __torch_function__(self, func, types, args, kwargs) 315 316 def __torch_function__(self, func, types, args=(), kwargs=None): --&gt; 317 with torch._C.DisableTorchFunction(): ret = _convert(func(*args, **(kwargs or {})), self.__class__) 318 if isinstance(ret, TensorBase): ret.set_meta(self, as_copy=True) 319 return ret KeyboardInterrupt: . If you hit a &quot;CUDA out of memory error&quot; after running this cell, click on the menu Kernel, then restart. Instead of executing the cell above, copy and paste the following code in it: . from fastai.text.all import * dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid=&#39;test&#39;, bs=32) learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy) learn.fine_tune(4, 1e-2) . This reduces the batch size to 32 (we will explain this later). If you keep hitting the same error, change 32 to 16. . learn.predict(&quot;I really liked that movie!&quot;) . (&#39;neg&#39;, TensorText(0), TensorText([0.5623, 0.4377])) . Sidebar: The Order Matters . End sidebar . from fastai.tabular.all import * path = untar_data(URLs.ADULT_SAMPLE) dls = TabularDataLoaders.from_csv(path/&#39;adult.csv&#39;, path=path, y_names=&quot;salary&quot;, cat_names = [&#39;workclass&#39;, &#39;education&#39;, &#39;marital-status&#39;, &#39;occupation&#39;, &#39;relationship&#39;, &#39;race&#39;], cont_names = [&#39;age&#39;, &#39;fnlwgt&#39;, &#39;education-num&#39;], procs = [Categorify, FillMissing, Normalize]) learn = tabular_learner(dls, metrics=accuracy) . learn.fit_one_cycle(3) . epoch train_loss valid_loss accuracy time . 0 | 0.367843 | 0.357764 | 0.832924 | 00:07 | . 1 | 0.362614 | 0.354566 | 0.831849 | 00:07 | . 2 | 0.344427 | 0.349152 | 0.837838 | 00:08 | . from fastai.collab import * path = untar_data(URLs.ML_SAMPLE) dls = CollabDataLoaders.from_csv(path/&#39;ratings.csv&#39;) learn = collab_learner(dls, y_range=(0.5,5.5)) learn.fine_tune(20) . epoch train_loss valid_loss time . 0 | 1.523609 | 1.353591 | 00:00 | . epoch train_loss valid_loss time . 0 | 1.377288 | 1.306087 | 00:00 | . 1 | 1.314181 | 1.222488 | 00:00 | . 2 | 1.191319 | 1.065583 | 00:00 | . 3 | 1.010563 | 0.854901 | 00:00 | . 4 | 0.818294 | 0.725766 | 00:00 | . 5 | 0.699861 | 0.688180 | 00:00 | . 6 | 0.650566 | 0.677446 | 00:00 | . 7 | 0.629411 | 0.673032 | 00:00 | . 8 | 0.609606 | 0.668113 | 00:00 | . 9 | 0.606750 | 0.663429 | 00:00 | . 10 | 0.594681 | 0.660590 | 00:00 | . 11 | 0.583541 | 0.656942 | 00:00 | . 12 | 0.574227 | 0.654432 | 00:00 | . 13 | 0.567019 | 0.651633 | 00:00 | . 14 | 0.548985 | 0.650612 | 00:00 | . 15 | 0.544749 | 0.649310 | 00:00 | . 16 | 0.542530 | 0.648421 | 00:00 | . 17 | 0.546451 | 0.648087 | 00:00 | . 18 | 0.544443 | 0.647970 | 00:00 | . 19 | 0.539262 | 0.647940 | 00:00 | . learn.show_results() . userId movieId rating rating_pred . 0 83.0 | 64.0 | 3.5 | 3.697940 | . 1 27.0 | 42.0 | 3.0 | 3.154352 | . 2 73.0 | 48.0 | 4.0 | 3.848425 | . 3 80.0 | 31.0 | 3.5 | 3.996437 | . 4 12.0 | 13.0 | 4.0 | 3.767331 | . 5 30.0 | 20.0 | 3.0 | 4.216630 | . 6 85.0 | 58.0 | 5.0 | 4.794383 | . 7 27.0 | 54.0 | 2.5 | 4.053595 | . 8 66.0 | 44.0 | 4.0 | 3.077837 | . Sidebar: Datasets: Food for Models . End sidebar . Validation Sets and Test Sets . Use Judgment in Defining Test Sets . A Choose Your Own Adventure moment . Questionnaire . It can be hard to know in pages and pages of prose what the key things are that you really need to focus on and remember. So, we&#39;ve prepared a list of questions and suggested steps to complete at the end of each chapter. All the answers are in the text of the chapter, so if you&#39;re not sure about anything here, reread that part of the text and make sure you understand it. Answers to all these questions are also available on the book&#39;s website. You can also visit the forums if you get stuck to get help from other folks studying this material. . For more questions, including detailed answers and links to the video timeline, have a look at Radek Osmulski&#39;s aiquizzes. . Do you need these for deep learning? . Lots of math T / F | Lots of data T / F | Lots of expensive computers T / F | A PhD T / F | . | Name five areas where deep learning is now the best in the world. . | What was the name of the first device that was based on the principle of the artificial neuron? | Based on the book of the same name, what are the requirements for parallel distributed processing (PDP)? | What were the two theoretical misunderstandings that held back the field of neural networks? | What is a GPU? | Open a notebook and execute a cell containing: 1+1. What happens? | Follow through each cell of the stripped version of the notebook for this chapter. Before executing each cell, guess what will happen. | Complete the Jupyter Notebook online appendix. | Why is it hard to use a traditional computer program to recognize images in a photo? | What did Samuel mean by &quot;weight assignment&quot;? | What term do we normally use in deep learning for what Samuel called &quot;weights&quot;? | Draw a picture that summarizes Samuel&#39;s view of a machine learning model. | Why is it hard to understand why a deep learning model makes a particular prediction? | What is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy? | What do you need in order to train a model? | How could a feedback loop impact the rollout of a predictive policing model? | Do we always have to use 224×224-pixel images with the cat recognition model? | What is the difference between classification and regression? | What is a validation set? What is a test set? Why do we need them? | What will fastai do if you don&#39;t provide a validation set? | Can we always use a random sample for a validation set? Why or why not? | What is overfitting? Provide an example. | What is a metric? How does it differ from &quot;loss&quot;? | How can pretrained models help? | What is the &quot;head&quot; of a model? | What kinds of features do the early layers of a CNN find? How about the later layers? | Are image models only useful for photos? | What is an &quot;architecture&quot;? | What is segmentation? | What is y_range used for? When do we need it? | What are &quot;hyperparameters&quot;? | What&#39;s the best way to avoid failures when using AI in an organization? | Further Research . Each chapter also has a &quot;Further Research&quot; section that poses questions that aren&#39;t fully answered in the text, or gives more advanced assignments. Answers to these questions aren&#39;t on the book&#39;s website; you&#39;ll need to do your own research! . Why is a GPU useful for deep learning? How is a CPU different, and why is it less effective for deep learning? | Try to think of three areas where feedback loops might impact the use of machine learning. See if you can find documented examples of that happening in practice. |",
            "url": "https://chrismilleruk.github.io/fastpages/2021/01/06/fastai-01_intro.html",
            "relUrl": "/2021/01/06/fastai-01_intro.html",
            "date": " • Jan 6, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://chrismilleruk.github.io/fastpages/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://chrismilleruk.github.io/fastpages/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! My name is Chris :rocket: . That’s all you need to know for now, but I’ll put more here later. . Find me online using the links in the footer or see some recent tweets: . Tweets by chrismilleruk btw. this website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://chrismilleruk.github.io/fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://chrismilleruk.github.io/fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}